{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ec6588",
   "metadata": {},
   "source": [
    "# Create a new spark Datasource\n",
    "Use this notebook to configure a new spark Datasource and add it to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc98b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import os\n",
    "from great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\n",
    "from ruamel import yaml\n",
    "\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import BatchRequest, RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import (\n",
    "    DataContextConfig,\n",
    "    InMemoryStoreBackendDefaults,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7f253",
   "metadata": {},
   "source": [
    "## Customize Your Datasource Configuration\n",
    "\n",
    "**If you are new to Great Expectations Datasources,** you should check out our [how-to documentation](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_overview)\n",
    "\n",
    "**My configuration is not so simple - are there more advanced options?**\n",
    "Glad you asked! Datasources are versatile. Please see our [How To Guides](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_overview)!\n",
    "\n",
    "Give your datasource a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a339aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_name = \"my_spark_dataframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc33bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_backend_defaults = InMemoryStoreBackendDefaults()\n",
    "data_context_config = DataContextConfig(\n",
    "    store_backend_defaults=store_backend_defaults,\n",
    "    checkpoint_store_name=store_backend_defaults.checkpoint_store_name,\n",
    ")\n",
    "context = BaseDataContext(project_config=data_context_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c419925",
   "metadata": {},
   "source": [
    "### For files based Datasources:\n",
    "Here we are creating an example configuration.  The configuration contains an **InferredAssetFilesystemDataConnector** which will add a Data Asset for each file in the base directory you provided. It also contains a **RuntimeDataConnector** which can accept filepaths.   This is just an example, and you may customize this as you wish!\n",
    "\n",
    "Also, if you would like to learn more about the **DataConnectors** used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on [InferredAssetDataConnectors](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_an_inferredassetdataconnector) and [RuntimeDataConnectors](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_a_runtimedataconnector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_yaml = f\"\"\"\n",
    "name: my_spark_dataframe\n",
    "class_name: Datasource\n",
    "execution_engine:\n",
    "    class_name: SparkDFExecutionEngine\n",
    "data_connectors:\n",
    "    default_runtime_data_connector_name:\n",
    "        class_name: RuntimeDataConnector\n",
    "        batch_identifiers:\n",
    "            - batch_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd051e1",
   "metadata": {},
   "source": [
    "# Test Your Datasource Configuration\n",
    "Here we will test your Datasource configuration to make sure it is valid.\n",
    "\n",
    "This `test_yaml_config()` function is meant to enable fast dev loops. **If your\n",
    "configuration is correct, this cell will show you some snippets of the data\n",
    "assets in the data source.** You can continually edit your Datasource config\n",
    "yaml and re-run the cell to check until the new config is valid.\n",
    "\n",
    "If you instead wish to use python instead of yaml to configure your Datasource,\n",
    "you can use `context.add_datasource()` and specify all the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.test_yaml_config(datasource_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998944b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.add_datasource(**yaml.load(datasource_yaml))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7b4d3",
   "metadata": {},
   "source": [
    "#### Conneting the spark local and create my dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e47434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 16:49:29 WARN Utils: Your hostname, dell-inspiron resolves to a loopback address: 127.0.1.1; using 172.18.181.232 instead (on interface eth0)\n",
      "22/08/27 16:49:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 16:49:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "context = ge.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651b6fff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/junin/Desktop/Estudos/great-expectations/POC/great_expectations/uncommitted/datasource_new.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/junin/Desktop/Estudos/great-expectations/POC/great_expectations/uncommitted/datasource_new.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mspark://spark:7077\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mgreat_expectation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:275\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m--> 275\u001b[0m         \u001b[39mgetattr\u001b[39;49m(session\u001b[39m.\u001b[39;49m_jvm, \u001b[39m\"\u001b[39;49m\u001b[39mSparkSession$\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1707\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1709\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1710\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1711\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1712\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1713\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1714\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://spark:7077\").appName(\"great_expectation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58a3a4",
   "metadata": {},
   "source": [
    "#### Load the file and create Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fd04ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"../../data/Educacao_Basica_2018 - Matricula_Norte.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30c358",
   "metadata": {},
   "source": [
    "#### TEST YOUT NEW DATASOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a781244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb16b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=datasource_name,\n",
    "    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "    data_asset_name=\"my_spark_dataframe\",  # This can be anything that identifies this data_asset for you\n",
    "    batch_identifiers={\"batch_id\": \"default_identifier\"},\n",
    "    runtime_parameters={\"batch_data\": df},  # Your dataframe goes here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e4970",
   "metadata": {},
   "source": [
    "#### the  load data into the Validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a151a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"ge_cloud_id\": null,\n",
       "  \"data_asset_type\": null,\n",
       "  \"meta\": {\n",
       "    \"great_expectations_version\": \"0.15.17\"\n",
       "  },\n",
       "  \"expectation_suite_name\": \"spark_educacao_basica_suite\",\n",
       "  \"expectations\": []\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.create_expectation_suite(\n",
    "    expectation_suite_name=\"spark_educacao_basica_suite\", overwrite_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5898b",
   "metadata": {},
   "source": [
    "## Save Your Datasource Configuration\n",
    "Here we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that `overwrite_existing` defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your `great_expectations.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2afb22c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a96988c4474c8e9c11d8259b7951b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 17:03:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "  NU_ANO_CENSO                          ID_ALUNO ID_MATRICULA NU_DIA NU_MES  \\\n",
      "0         2018  B587FA609EFCFCAF6786CE479E30E59E    281920109     27      9   \n",
      "1         2018  D1C0B9634BCD926A0B7D35ED1C40C785    168422655      7      7   \n",
      "2         2018  AEC84DC24F21E017F4C076E7C0542568    168926791     21      9   \n",
      "3         2018  C938C2CD9F950AD5C17D12C630B5D0B1    281917630     19     10   \n",
      "4         2018  75FE0FA9F9877DE92FFB40E79E8C1557    281909633     15      9   \n",
      "\n",
      "  NU_ANO NU_IDADE_REFERENCIA NU_IDADE NU_DURACAO_TURMA  \\\n",
      "0   1989                  28       29              270   \n",
      "1   2010                   7        8              255   \n",
      "2   2010                   7        8              255   \n",
      "3   2014                   3        4              255   \n",
      "4   2010                   7        8              255   \n",
      "\n",
      "  NU_DUR_ATIV_COMP_MESMA_REDE  ... IN_CONVENIADA_PP TP_CONVENIO_PODER_PUBLICO  \\\n",
      "0                           0  ...             None                      None   \n",
      "1                           0  ...             None                      None   \n",
      "2                           0  ...             None                      None   \n",
      "3                           0  ...             None                      None   \n",
      "4                           0  ...             None                      None   \n",
      "\n",
      "  IN_MANT_ESCOLA_PRIVADA_EMP IN_MANT_ESCOLA_PRIVADA_ONG  \\\n",
      "0                       None                       None   \n",
      "1                       None                       None   \n",
      "2                       None                       None   \n",
      "3                       None                       None   \n",
      "4                       None                       None   \n",
      "\n",
      "  IN_MANT_ESCOLA_PRIVADA_SIND IN_MANT_ESCOLA_PRIVADA_SIST_S  \\\n",
      "0                        None                          None   \n",
      "1                        None                          None   \n",
      "2                        None                          None   \n",
      "3                        None                          None   \n",
      "4                        None                          None   \n",
      "\n",
      "  IN_MANT_ESCOLA_PRIVADA_S_FINS TP_REGULAMENTACAO TP_LOCALIZACAO_DIFERENCIADA  \\\n",
      "0                          None                 0                           0   \n",
      "1                          None                 1                           0   \n",
      "2                          None                 1                           0   \n",
      "3                          None                 1                           0   \n",
      "4                          None                 1                           0   \n",
      "\n",
      "  IN_EDUCACAO_INDIGENA  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "3                    0  \n",
      "4                    0  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "validator = context.get_validator(\n",
    "    batch_request=batch_request, expectation_suite_name=\"spark_educacao_basica_suite\"\n",
    ")\n",
    "print(validator.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4e35caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'execution_engine': {'module_name': 'great_expectations.execution_engine',\n",
       "   'class_name': 'SparkDFExecutionEngine'},\n",
       "  'data_connectors': {'default_runtime_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n",
       "    'class_name': 'RuntimeDataConnector',\n",
       "    'batch_identifiers': ['batch_id']}},\n",
       "  'module_name': 'great_expectations.datasource',\n",
       "  'class_name': 'Datasource',\n",
       "  'name': 'my_spark_dataframe'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.list_datasources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7006d90",
   "metadata": {},
   "source": [
    "#### Create expectations in my suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ab5d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = context.get_expectation_suite(\"spark_educacao_basica_suite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04333e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset\n",
    "\n",
    "my_df = SparkDFDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5052b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                         (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 17:04:12 WARN MemoryStore: Not enough space to cache rdd_66_7 in memory! (computed 38.8 MiB so far)\n",
      "22/08/27 17:04:12 WARN BlockManager: Persisting block rdd_66_7 to disk instead.\n",
      "22/08/27 17:04:21 WARN MemoryStore: Not enough space to cache rdd_66_2 in memory! (computed 73.3 MiB so far)\n",
      "22/08/27 17:04:21 WARN BlockManager: Persisting block rdd_66_2 to disk instead.\n",
      "22/08/27 17:04:22 WARN MemoryStore: Not enough space to cache rdd_66_5 in memory! (computed 75.2 MiB so far)\n",
      "22/08/27 17:04:22 WARN MemoryStore: Not enough space to cache rdd_66_4 in memory! (computed 74.4 MiB so far)\n",
      "22/08/27 17:04:22 WARN BlockManager: Persisting block rdd_66_4 to disk instead.\n",
      "22/08/27 17:04:22 WARN MemoryStore: Not enough space to cache rdd_66_1 in memory! (computed 73.0 MiB so far)\n",
      "22/08/27 17:04:22 WARN BlockManager: Persisting block rdd_66_1 to disk instead.\n",
      "22/08/27 17:04:22 WARN BlockManager: Persisting block rdd_66_5 to disk instead.\n",
      "22/08/27 17:04:23 WARN MemoryStore: Not enough space to cache rdd_66_0 in memory! (computed 74.1 MiB so far)\n",
      "22/08/27 17:04:23 WARN BlockManager: Persisting block rdd_66_0 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                         (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_4 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 ERROR Executor: Exception in task 4.0 in stage 7.0 (TID 11)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:361)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:57)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3345/0x00000008013794a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2298/0x0000000801862008.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "22/08/27 17:05:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 7.0 (TID 11),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:361)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:57)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3345/0x00000008013794a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2298/0x0000000801862008.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 4.0 in stage 7.0 (TID 11) (172.18.181.232 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:361)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:161)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:57)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:104)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3345/0x00000008013794a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2298/0x0000000801862008.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\n",
      "22/08/27 17:05:11 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@11194e10 rejected from java.util.concurrent.ThreadPoolExecutor@3ad49372[Shutting down, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 7]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2057)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:827)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1357)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "22/08/27 17:05:11 ERROR TaskSetManager: Task 4 in stage 7.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                         (0 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_6 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_7 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 6.0 in stage 7.0 (TID 13) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 14) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_3 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_1 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_5 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 10) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 8) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_0 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN BlockManager: Putting block rdd_66_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "22/08/27 17:05:11 WARN BlockManager: Block rdd_66_2 could not be removed as it was not found on disk or in memory\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 5.0 in stage 7.0 (TID 12) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n",
      "22/08/27 17:05:11 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 9) (172.18.181.232 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12481/3104309534.py\", line 1, in <cell line: 1>\n",
      "    my_df.expect_column_median_to_be_between(column=[\"NU_DURACAO_TURMA\"], min_value=20, max_value=100)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/great_expectations/data_asset/util.py\", line 76, in f\n",
      "    return self.mthd(obj, *args, **kwargs)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/great_expectations/data_asset/data_asset.py\", line 289, in wrapper\n",
      "    raise err\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/great_expectations/data_asset/data_asset.py\", line 276, in wrapper\n",
      "    return_obj = func(self, **evaluation_args)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/great_expectations/dataset/dataset.py\", line 116, in inner_wrapper\n",
      "    element_count = self.get_row_count()\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/great_expectations/dataset/sparkdf_dataset.py\", line 635, in get_row_count\n",
      "    return self.spark_df.count()\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 804, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/airtonjr/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/mnt/c/Users/junin/Desktop/Estudos/great-expectations/POC/great_expectations/uncommitted/datasource_new.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/junin/Desktop/Estudos/great-expectations/POC/great_expectations/uncommitted/datasource_new.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m my_df\u001b[39m.\u001b[39;49mexpect_column_median_to_be_between(column\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mNU_DURACAO_TURMA\u001b[39;49m\u001b[39m\"\u001b[39;49m], min_value\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, max_value\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/junin/Desktop/Estudos/great-expectations/POC/great_expectations/uncommitted/datasource_new.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m my_df\u001b[39m.\u001b[39mexpect_column_values_to_be_in_set(column\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mNU_IDADE\u001b[39m\u001b[39m\"\u001b[39m], value_set\u001b[39m=\u001b[39m[\u001b[39m29\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m20\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/great_expectations/data_asset/util.py:76\u001b[0m, in \u001b[0;36mDocInherit.__get__.<locals>.f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m@wraps\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmthd, assigned\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmthd(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/great_expectations/data_asset/data_asset.py:289\u001b[0m, in \u001b[0;36mDataAsset.expectation.<locals>.outer_wrapper.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m             \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/great_expectations/data_asset/data_asset.py:276\u001b[0m, in \u001b[0;36mDataAsset.expectation.<locals>.outer_wrapper.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     return_obj \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mevaluation_args)\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_obj, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/great_expectations/dataset/dataset.py:116\u001b[0m, in \u001b[0;36mMetaDataset.column_aggregate_expectation.<locals>.inner_wrapper\u001b[0;34m(self, column, result_format, row_condition, condition_parser, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(row_condition, parser\u001b[39m=\u001b[39mcondition_parser)\u001b[39m.\u001b[39mreset_index(\n\u001b[1;32m    113\u001b[0m         drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[0;32m--> 116\u001b[0m element_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_row_count()\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/great_expectations/dataset/sparkdf_dataset.py:635\u001b[0m, in \u001b[0;36mSparkDFDataset.get_row_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_row_count\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 635\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspark_df\u001b[39m.\u001b[39;49mcount()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[39m2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2004\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2002\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2004\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2005\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2006\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    532\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    533\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    535\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    539\u001b[0m }\n\u001b[1;32m    541\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    542\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "my_df.expect_column_median_to_be_between(column=[\"NU_DURACAO_TURMA\"], min_value=20, max_value=100)\n",
    "my_df.expect_column_values_to_be_in_set(column=[\"NU_IDADE\"], value_set=[29, 30, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c9ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
